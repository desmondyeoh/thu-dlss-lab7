{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-69e42698ebc0>, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-69e42698ebc0>\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    x = tf.placeholder(tf.float32, shape=[None, ......], name='x')\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorlayer as tl\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from LoadData import LoadData\n",
    "from tensorlayer.utils import dict_to_one\n",
    "import time\n",
    "\n",
    "def get_session(gpu_fraction=0.2):\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction,\n",
    "                                allow_growth=True)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "def minibatches(inputs=None, inputs2=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], inputs2[excerpt], targets[excerpt]\n",
    "\n",
    "def fit(sess, network, train_op, cost, X_train, X_train2, y_train, x, x_2, y_, acc=None, batch_size=100,\n",
    "        n_epoch=100, print_freq=5, X_val=None, X_val2=None, y_val=None, eval_train=True,\n",
    "        tensorboard=False, tensorboard_epoch_freq=5, tensorboard_weight_histograms=True, tensorboard_graph_vis=True):\n",
    "    assert X_train.shape[0] >= batch_size, \"Number of training examples should be bigger than the batch size\"\n",
    "    print(\"Start training the network ...\")\n",
    "    start_time_begin = time.time()\n",
    "    tensorboard_train_index, tensorboard_val_index = 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        loss_ep = 0; n_step = 0\n",
    "        for X_train_a, X_train_b, y_train_a in minibatches(X_train, X_train2, y_train,\n",
    "                                                    batch_size, shuffle=True):\n",
    "            feed_dict = {x: X_train_a, x_2:X_train_b, y_: y_train_a}\n",
    "            feed_dict.update( network.all_drop )    # enable noise layers\n",
    "            loss, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n",
    "            #loss, _, y_prediction = sess.run([cost, train_op, y_], feed_dict=feed_dict)\n",
    "            loss_ep += loss\n",
    "            n_step += 1\n",
    "        loss_ep = loss_ep/ n_step\n",
    "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "                if eval_train is True:\n",
    "                    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "                    for X_train_a, X_train_b, y_train_a in minibatches(\n",
    "                                            X_train, X_train2, y_train, batch_size, shuffle=True):\n",
    "                        dp_dict = dict_to_one( network.all_drop )    # disable noise layers\n",
    "                        feed_dict = {x: X_train_a, x_2:X_train_b, y_: y_train_a}\n",
    "                        feed_dict.update(dp_dict)\n",
    "                        if acc is not None:\n",
    "                            err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                            train_acc += ac\n",
    "                        else:\n",
    "                            err = sess.run(cost, feed_dict=feed_dict)\n",
    "                        train_loss += err;  n_batch += 1\n",
    "                    print(\"   train loss: %f\" % (train_loss/ n_batch))\n",
    "                    if acc is not None:\n",
    "                        print(\"   train acc: %f\" % (train_acc/ n_batch))\n",
    "                val_loss, val_acc, n_batch = 0, 0, 0\n",
    "                for X_val_a, X_val_b, y_val_a in minibatches(\n",
    "                                            X_val, X_val2, y_val, batch_size, shuffle=True):\n",
    "                    dp_dict = dict_to_one( network.all_drop )    # disable noise layers\n",
    "                    feed_dict = {x: X_val_a, x_2:X_val_b, y_: y_val_a}\n",
    "                    feed_dict.update(dp_dict)\n",
    "                    if acc is not None:\n",
    "                        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                        # y_predi = y_predi.append(y_pred)\n",
    "                        val_acc += ac\n",
    "                    else:\n",
    "                        err = sess.run([cost], feed_dict=feed_dict)\n",
    "                        # y_predi = y_predi.append(y_pred)\n",
    "                    val_loss += err; n_batch += 1\n",
    "                print(\"   val loss: %f\" % (val_loss/ n_batch))\n",
    "                if acc is not None:\n",
    "                    print(\"   val acc: %f\" % (val_acc/ n_batch))\n",
    "            else:\n",
    "                print(\"Epoch %d of %d took %fs, loss %f\" % (epoch + 1, n_epoch, time.time() - start_time, loss_ep))\n",
    "        print(\"Epoch %d of %d took %fs, loss %f\" % (epoch + 1, n_epoch, time.time() - start_time, loss_ep))\n",
    "    print(\"Total training time: %fs\" % (time.time() - start_time_begin))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    data_loader = LoadData(root_path=\"data/rgbd-dataset_eval\")\n",
    "    all_train_rgb_samples, all_train_depth_samples, all_train_labels, all_test_rgb_samples, all_test_depth_samples, all_test_labels = data_loader.load_data()\n",
    "    session = get_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # define placeholder \n",
    "    x = tf.placeholder(tf.float32, shape=[None,......], name='x')\n",
    "    x_depth = tf.placeholder(tf.float32, shape=[None,......], name='x_depth')\n",
    "    y_ = tf.placeholder(tf.int64, shape=[......], name='y_')\n",
    "\n",
    "    # define model(fill your code)\n",
    "\n",
    "    .............................\n",
    "    .............................\n",
    "    .............................\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # define loss\n",
    "    y = network.outputs\n",
    "    cost = tl.cost.cross_entropy(y, y_, name=\"cost\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    y_op = tf.argmax(tf.nn.softmax(y), 1)\n",
    "\n",
    "    # define optimizer\n",
    "    train_params = network.all_params\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.9, beta2=0.999,\n",
    "                                      epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)\n",
    "\n",
    "    # initialize\n",
    "    tl.layers.initialize_global_variables(session)\n",
    "\n",
    "    # list model info\n",
    "    # network.print_params()\n",
    "    # network.print_layers()\n",
    "\n",
    "    # train and test model\n",
    "    fit(session, network, train_op, cost, np.array(all_train_rgb_samples), np.array(all_train_depth_samples), np.array(all_train_labels), x, x_depth, y_,\n",
    "                 acc=acc, batch_size=100, n_epoch=500, print_freq=1,\n",
    "                 X_val=np.array(all_test_rgb_samples), X_val2=np.array(all_test_depth_samples), y_val=np.array(all_test_labels), eval_train=True)\n",
    "\n",
    "    session.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
